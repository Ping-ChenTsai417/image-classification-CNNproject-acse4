{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycm in /opt/firedrake/lib/python3.6/site-packages (2.6)\n",
      "Requirement already satisfied: livelossplot in /opt/firedrake/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: albumentations in /opt/firedrake/lib/python3.6/site-packages (0.4.5)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/firedrake/lib/python3.6/site-packages (from pycm) (1.18.2)\n",
      "Requirement already satisfied: art>=1.8 in /opt/firedrake/lib/python3.6/site-packages (from pycm) (4.5)\n",
      "Requirement already satisfied: ipython in /opt/firedrake/lib/python3.6/site-packages (from livelossplot) (7.13.0)\n",
      "Requirement already satisfied: matplotlib; python_version >= \"3.6\" in /opt/firedrake/lib/python3.6/site-packages (from livelossplot) (3.2.1)\n",
      "Requirement already satisfied: imgaug<0.2.7,>=0.2.5 in /opt/firedrake/lib/python3.6/site-packages (from albumentations) (0.2.6)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/firedrake/lib/python3.6/site-packages (from albumentations) (4.2.0.34)\n",
      "Requirement already satisfied: PyYAML in /opt/firedrake/lib/python3.6/site-packages (from albumentations) (5.3.1)\n",
      "Requirement already satisfied: scipy in /opt/firedrake/lib/python3.6/site-packages (from albumentations) (1.4.1)\n",
      "Requirement already satisfied: coverage>=4.1 in /opt/firedrake/lib/python3.6/site-packages (from art>=1.8->pycm) (4.4)\n",
      "Requirement already satisfied: pickleshare in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (4.3.3)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (0.16.0)\n",
      "Requirement already satisfied: decorator in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (4.4.2)\n",
      "Requirement already satisfied: backcall in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (2.6.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (46.1.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (3.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/firedrake/lib/python3.6/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/firedrake/lib/python3.6/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/firedrake/lib/python3.6/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/firedrake/lib/python3.6/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (1.1.0)\n",
      "Requirement already satisfied: six in /opt/firedrake/lib/python3.6/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.14.0)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /opt/firedrake/lib/python3.6/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n",
      "Requirement already satisfied: ipython-genutils in /opt/firedrake/lib/python3.6/site-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\n",
      "Requirement already satisfied: parso>=0.5.2 in /opt/firedrake/lib/python3.6/site-packages (from jedi>=0.10->ipython->livelossplot) (0.6.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/firedrake/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython->livelossplot) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /opt/firedrake/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->livelossplot) (0.1.9)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /opt/firedrake/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.0.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/firedrake/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/firedrake/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/firedrake/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/opt/firedrake/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "!pip install pycm livelossplot albumentations\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "use_albumentations = False\n",
    "\n",
    "train_whole_model = False\n",
    "train_last_layer = True\n",
    "train_some_layers = False\n",
    "\n",
    "use_googlenet = False\n",
    "use_resnet50 = False\n",
    "use_inceptionV3 = True \n",
    "use_resnet18 = False\n",
    "\n",
    "# Auxliary output for inceptionV3\n",
    "Auxliary_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda installed! Running on GPU!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score # this allows us to evaluate our model at every iteration\n",
    "from sklearn.metrics import f1_score # this allows us to evaluate our validation accuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit # this allows us to create a random validation split\n",
    "\n",
    "# These imports help plot the convergence and create the confusion matrix\n",
    "from livelossplot import PlotLosses\n",
    "from pycm import *\n",
    "\n",
    "# These imports help us create models and datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# This allows me to create my own custom dataset\n",
    "from torch.utils.data import Dataset \n",
    "from torchvision.datasets.folder import *\n",
    "\n",
    "# This allows me to import pretrained models for transfer learning\n",
    "import torchvision.models as models\n",
    "\n",
    "# This allows me to do a number of transforms for data augmentation later on\n",
    "from torchvision.transforms import Compose, ToTensor, ColorJitter, Resize, Normalize, RandomApply, RandomChoice, RandomRotation, RandomCrop, RandomResizedCrop, RandomHorizontalFlip, RandomAffine, ToPILImage\n",
    "\n",
    "# These imports help us write the submission file\n",
    "import json, csv\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# We will be using albumentations to perform data augmentation\n",
    "if (use_albumentations):\n",
    "    from albumentations import Compose\n",
    "    import albumentations.augmentations.transforms as transforms\n",
    "\n",
    "# This helps us keep a copy of model state dicts\n",
    "import copy\n",
    "\n",
    "# To display random images\n",
    "from random import randrange\n",
    "\n",
    "# Enable hardware acceleration\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda:2'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The means of our entire training set, calculated in another file:\n",
    "means = [0.4805, 0.4483, 0.3978]\n",
    "stds = [0.2177, 0.2138, 0.2136]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewritten to help separate data from labels\n",
    "def my_make_dataset(directory, class_to_idx, extensions=None, is_valid_file=None):\n",
    "    data = []\n",
    "    targets = []\n",
    "    directory = os.path.expanduser(directory)\n",
    "    both_none = extensions is None and is_valid_file is None\n",
    "    both_something = extensions is not None and is_valid_file is not None\n",
    "    if both_none or both_something:\n",
    "        raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
    "    if extensions is not None:\n",
    "        def is_valid_file(x):\n",
    "            return has_file_allowed_extension(x, extensions)\n",
    "    for target_class in sorted(class_to_idx.keys()):\n",
    "        class_index = class_to_idx[target_class]\n",
    "        target_dir = os.path.join(directory, target_class)\n",
    "        if not os.path.isdir(target_dir):\n",
    "            continue\n",
    "        for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
    "            for fname in sorted(fnames):\n",
    "                path = os.path.join(root, fname)\n",
    "                if is_valid_file(path):\n",
    "                    item = path, class_index\n",
    "                    data.append(path)\n",
    "                    targets.append(class_index)\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "# A custom dataset class that will work with albumentations because ImageFolder does not\n",
    "class AlbumentationImageFolder(ImageFolder):\n",
    "    def __init__(self, root, extensions=IMG_EXTENSIONS, transform=None,\n",
    "                 target_transform=None, is_valid_file=None, augmentation=None):\n",
    "        super(ImageFolder, self).__init__(root, default_loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
    "                                          transform=transform,\n",
    "                                          target_transform=target_transform,\n",
    "                                          is_valid_file=is_valid_file)\n",
    "        \n",
    "        classes, class_to_idx = self._find_classes(self.root)\n",
    "        data, targets = my_make_dataset(self.root, class_to_idx, extensions, is_valid_file)\n",
    "        self.imgs = self.samples\n",
    "        self.data = [ToTensor()(self.loader(data[i])) for i in range(len(data))]\n",
    "        self.targets = targets\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        sample = self.loader(path)\n",
    "        if self.augmentation is not None:\n",
    "            sample = self.augmentation(image=np.asarray(sample))['image']\n",
    "            sample = np.transpose(sample, (2, 0, 1)).astype(np.float32)\n",
    "            sample = torch.tensor(sample, dtype=torch.float)\n",
    "            \n",
    "        return sample, target\n",
    "    \n",
    "class CustomImageTensorDatasetNA(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Tensor): A tensor containing the data e.g. images\n",
    "            targets (Tensor): A tensor containing all the labels\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, label = self.data[idx], self.targets[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample, label\n",
    "    \n",
    "class CustomImageTensorDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Tensor): A tensor containing the data e.g. images\n",
    "            targets (Tensor): A tensor containing all the labels\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, label = self.data[idx], self.targets[idx]\n",
    "        if self.transform:\n",
    "            sample = ToPILImage()(sample).convert(\"RGB\")\n",
    "            sample = self.transform(image=np.array(sample, dtype = np.uint8))['image']\n",
    "            sample = np.transpose(sample, (2, 0, 1)).astype(np.float32)\n",
    "            sample = torch.tensor(sample, dtype=torch.float)\n",
    "\n",
    "        return sample, label\n",
    "    \n",
    "# A custom dataset class that retains filenames for use in creating the csv file\n",
    "class ImageFolderWithPaths(ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # We only need to override the __getitem__ method\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "    \n",
    "from torchvision.datasets.folder import *\n",
    "class AlbumentationImageFolder(ImageFolder):\n",
    "    '''Custom ImageFolder to implement albumentations transforms as well as pytorch transforms'''\n",
    "    \n",
    "    def __init__(self, root, extensions=IMG_EXTENSIONS, transform=None,\n",
    "                 target_transform=None, is_valid_file=None, augmentation=None):\n",
    "        super(ImageFolder, self).__init__(root, default_loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
    "                                          transform=transform,\n",
    "                                          target_transform=target_transform,\n",
    "                                          is_valid_file=is_valid_file)\n",
    "        self.imgs = self.samples\n",
    "        self.augmentation = augmentation\n",
    "    def __getitem__(self, idx):\n",
    "        '''Returns a tuple of the image tensor and the label'''\n",
    "        path, target = self.samples[idx]\n",
    "        sample = np.asarray(self.loader(path))\n",
    "        if self.augmentation is not None:\n",
    "            sample = self.augmentation(image=sample)['image']\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        sample = np.transpose(sample, (2, 0, 1)).astype(np.float32)\n",
    "        sample = torch.tensor(sample, dtype=torch.float)\n",
    "            \n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "#     ToPILImage(),\n",
    "#     RandomApply([RandomChoice([RandomCrop(size=[64, 64], padding=4), RandomAffine(0, translate=(0, 0))])]),\n",
    "    RandomHorizontalFlip(),\n",
    "#     ColorJitter(brightness=0.1, contrast=0.05, saturation=2, hue=0.08),\n",
    "    Resize(299),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=means, std=stds), \n",
    "])\n",
    "\n",
    "validation_test_transform = Compose([\n",
    "#     ToPILImage(),\n",
    "    Resize(299),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=means, std=stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = ImageFolder(\"./train/\", transform = train_transform )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = ImageFolderWithPaths(\"./test/\", transform = validation_test_transform) # our custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "lr = 1e-3\n",
    "momentum = 0.8\n",
    "batch_size = 64\n",
    "test_batch_size = 100\n",
    "n_epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffleTorch = torch.zeros(100000, 3, 64, 64)\n",
    "# shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1).split(shuffleTorch, my_data.targets)\n",
    "# indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]\n",
    "# Split validation and training\n",
    "train_size = int(0.9 * len(my_data))\n",
    "validation_size = len(my_data) - train_size\n",
    "train_dataset, validation_dataset = random_split(my_data, [train_size, validation_size])\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "# Need it if we need output file\n",
    "test_loader = DataLoader(data_test, batch_size=test_batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_data.data = torch.stack(my_data.data)\n",
    "\n",
    "# X_train, y_train = my_data.data[indices[0]].float(), torch.from_numpy(np.array(my_data.targets)[indices[0]])\n",
    "# X_val, y_val = my_data.data[indices[1]].float(),  torch.from_numpy(np.array(my_data.targets)[indices[1]])\n",
    "# X_test, y_test = test_data.float(),  torch.from_numpy(np.array(test_data.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if (use_albumentations):\n",
    "#     data_train = CustomImageTensorDataset(X_train, y_train.long(), transform=Compose([\n",
    "#                         transforms.RandomContrast(),\n",
    "#                         transforms.HorizontalFlip(),\n",
    "#                         transforms.HueSaturationValue(),\n",
    "#                         transforms.Resize(224, 224),\n",
    "#                         transforms.Normalize(means, stds)\n",
    "#                         ]))\n",
    "#     data_validate = CustomImageTensorDataset(X_val, y_val.long(), transform=Compose([transforms.Resize(224, 224),\n",
    "#                         transforms.Normalize(means, stds)]))\n",
    "\n",
    "# else:\n",
    "#     data_train = CustomImageTensorDatasetNA(X_train, y_train.long(), transform=train_transform)\n",
    "#     data_validate = CustomImageTensorDatasetNA(X_val, y_val.long(), transform=validation_test_transform)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# validation_loader = DataLoader(validate_data, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "# # Need it if we need output file\n",
    "# test_loader = DataLoader(data_test, batch_size=test_batch_size, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_loader):\n",
    "    model.train()\n",
    "    train_loss, train_accuracy = 0, 0\n",
    "    for X, y in data_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        a2, aux = model(X.view(-1, 3, 299, 299))\n",
    "        loss1 = criterion(a2, y)\n",
    "        loss2 = criterion(aux, y)\n",
    "        loss = loss1 + 0.4 * loss2\n",
    "        loss.backward()\n",
    "        train_loss += loss*X.size(0)\n",
    "        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n",
    "        optimizer.step()  \n",
    "        \n",
    "    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)\n",
    "  \n",
    "def validate(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    validation_loss, validation_accuracy = 0., 0.\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 3, 299, 299))\n",
    "            loss = criterion(a2, y)\n",
    "            validation_loss += loss*X.size(0)\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n",
    "            \n",
    "    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)\n",
    "  \n",
    "# Evaluates our model's % accuracy\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    ys, y_preds = [], []\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 3, 299, 299))\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            ys.append(y.cpu().numpy())\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)\n",
    "# Generates predictions in the required format\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    files, y_preds = [], []\n",
    "    for X, y, z in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 3, 299, 299))\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            files.append(z)\n",
    "            \n",
    "    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n",
    "# ===================================================================================\n",
    "# The below code is for input 224 might not suit for Inception v3\n",
    "\n",
    "# Generates predictions by averaging two models\n",
    "def multimodel_predict(model_first, model_second, data_loader):\n",
    "    model_first.eval()\n",
    "    model_second.eval()\n",
    "    files, y_preds = [], []\n",
    "    for X, y, z in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model_first(X.view(-1, 3, 224, 224))\n",
    "            a3 = model_second(X.view(-1, 3, 224, 224))\n",
    "            a2 = (a2 + a3) / 2.\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            files.append(z)\n",
    "            \n",
    "    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n",
    "\n",
    "# Generates predictions by averaging 3 models\n",
    "def threemodel_predict(model_first, model_second, model_third, data_loader):\n",
    "    model_first.eval()\n",
    "    model_second.eval()\n",
    "    model_third.eval()\n",
    "    files, y_preds = [], []\n",
    "    for X, y, z in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model_first(X.view(-1, 3, 224, 224))\n",
    "            a3 = model_second(X.view(-1, 3, 224, 224))\n",
    "            a4 = model_third(X.view(-1, 3, 224, 224))\n",
    "            a2 = (a2 + a3 + a4) / 3.\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            files.append(z)\n",
    "            \n",
    "    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n",
    "\n",
    "# Checks the accuracy of our 3 model evaluation\n",
    "def threemodel_evaluate(model_first, model_second, model_third, data_loader):\n",
    "    model_first.eval()\n",
    "    model_second.eval()\n",
    "    model_third.eval()\n",
    "    files, y_preds = [], []\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model_first(X.view(-1, 3, 224, 224))\n",
    "            a3 = model_second(X.view(-1, 3, 224, 224))\n",
    "            a4 = model_third(X.view(-1, 3, 224, 224))\n",
    "            a2 = (a2 + a3 + a4) / 3.\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(y_preds, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /home/acse/.cache/torch/checkpoints/inception_v3_google-1a9a5a14.pth\n",
      "22.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "52.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "80.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "if (use_googlenet):\n",
    "    model = models.googlenet(pretrained=True).to(device)\n",
    "elif (use_resnet18):\n",
    "    model = models.resnet18(pretrained=True).to(device)\n",
    "elif (use_inceptionV3):\n",
    "    model = models.inception_v3(pretrained=True).to(device)\n",
    "    \n",
    "    # Auxliary output for InceptionV3\n",
    "    if Auxliary_output:\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model.AuxLogits.fc.in_features\n",
    "        model.AuxLogits.fc = nn.Linear(num_ftrs, 200)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs,200)\n",
    "    else:\n",
    "        model.aux_logits=False\n",
    "else:\n",
    "    model = models.resnet50(pretrained=True).to(device)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 200) # change number of output classes\n",
    "\n",
    "# uncomment to load previously created model:\n",
    "# model.load_state_dict(torch.load(\"./Googlenet_full_barely_augmented.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-82\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "if (train_some_layers):\n",
    "    ct = 0\n",
    "elif (train_last_layer):\n",
    "    ct = -100\n",
    "else:\n",
    "#     This number decides first ct number of \n",
    "    ct = 12\n",
    "    \n",
    "for child in model.children():\n",
    "    ct += 1\n",
    "    if ct < 12:\n",
    "#         freeze the preivous 11 layers\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "print(ct)\n",
    "\n",
    "if (train_last_layer):\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "\ttraining         \t (min:    0.111, max:    0.373, cur:    0.373)\n",
      "\tvalidation       \t (min:    0.418, max:    0.517, cur:    0.517)\n",
      "log loss\n",
      "\ttraining         \t (min:    6.544, max:    7.173, cur:    6.544)\n",
      "\tvalidation       \t (min:    4.043, max:    4.641, cur:    4.043)\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    logs = {}\n",
    "    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "    logs['' + 'log loss'] = train_loss.item()\n",
    "    logs['' + 'accuracy'] = train_accuracy.item()\n",
    "    \n",
    "    validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n",
    "    logs['val_' + 'log loss'] = validation_loss.item()\n",
    "    logs['val_' + 'accuracy'] = validation_accuracy.item()\n",
    "    \n",
    "    if validation_accuracy.item() > best_acc:\n",
    "        best_acc = validation_accuracy.item()\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = evaluate(model, validation_loader)\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictions, my_files = predict(model, test_loader)\n",
    "\n",
    "my_files_clean = [my_files[i][14:] for i in range(len(my_files))]\n",
    "\n",
    "with open('Inceptionv3_mom0_9.csv', 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow((\"Filename\", \"Label\"))\n",
    "    wr.writerows(zip(my_files_clean,my_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change your model name:\n",
    "model_save_name = 'Inceptionv3_1.pth'\n",
    "path = F\"./{model_save_name}\" \n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trial 1\n",
    "    -googlenet + resnet50\n",
    "    -Transform:\n",
    "        RandomApply([RandomChoice([RandomCrop(size=[64, 64], padding=4), RandomAffine(0, translate=(0.1, 0.1))])]),\n",
    "        RandomHorizontalFlip(),\n",
    "        ColorJitter(brightness=0.1, contrast=0.05, saturation=2, hue=0.08),\n",
    "    -Train on all dataset\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
