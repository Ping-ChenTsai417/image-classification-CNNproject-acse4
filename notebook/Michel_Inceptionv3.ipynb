{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install pycm livelossplot albumentations\n%pylab inline","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting pycm\n  Downloading pycm-2.7-py2.py3-none-any.whl (57 kB)\n\u001b[K     |████████████████████████████████| 57 kB 1.5 MB/s eta 0:00:011\n\u001b[?25hCollecting livelossplot\n  Downloading livelossplot-0.5.0-py3-none-any.whl (26 kB)\nRequirement already satisfied: albumentations in /opt/conda/lib/python3.7/site-packages (0.4.5)\nRequirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from pycm) (1.18.1)\nCollecting art>=1.8\n  Downloading art-4.7-py2.py3-none-any.whl (547 kB)\n\u001b[K     |████████████████████████████████| 547 kB 7.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from livelossplot) (7.13.0)\nRequirement already satisfied: matplotlib; python_version >= \"3.6\" in /opt/conda/lib/python3.7/site-packages (from livelossplot) (3.2.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.4.1)\nRequirement already satisfied: imgaug<0.2.7,>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.2.6)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations) (5.3.1)\nRequirement already satisfied: opencv-python>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (4.2.0.34)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (4.4.2)\nRequirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (0.15.2)\nRequirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (4.8.0)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (0.1.0)\nRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (46.1.3.post20200325)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (3.0.5)\nRequirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (4.3.3)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (0.7.5)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (2.6.1)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.8.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (0.10.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.4.7)\nRequirement already satisfied: scikit-image>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.14.0)\nRequirement already satisfied: parso>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython->livelossplot) (0.5.2)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->livelossplot) (0.6.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->livelossplot) (0.1.9)\nRequirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4)\nRequirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (5.4.1)\nRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.0)\nRequirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\nInstalling collected packages: art, pycm, livelossplot\nSuccessfully installed art-4.7 livelossplot-0.5.0 pycm-2.7\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nPopulating the interactive namespace from numpy and matplotlib\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"use_albumentations = False\n\ntrain_whole_model = True\ntrain_last_layer = False\ntrain_some_layers = False\n\nuse_googlenet = False\nuse_resnet50 = True\nuse_resnet18 = False","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score # this allows us to evaluate our model at every iteration\nfrom sklearn.metrics import f1_score # this allows us to evaluate our validation accuracy\nfrom sklearn.model_selection import StratifiedShuffleSplit # this allows us to create a random validation split\n\n# These imports help plot the convergence and create the confusion matrix\nfrom livelossplot import PlotLosses\nfrom pycm import *\n\n# These imports help us create models and datasets\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\n\n# This allows me to create my own custom dataset\nfrom torch.utils.data import Dataset \nfrom torchvision.datasets.folder import *\n\n# This allows me to import pretrained models for transfer learning\nimport torchvision.models as models\n\n# This allows me to do a number of transforms for data augmentation later on\nfrom torchvision.transforms import Compose, ToTensor, ColorJitter, Resize, Normalize, RandomApply, RandomChoice, RandomRotation, RandomCrop, RandomResizedCrop, RandomHorizontalFlip, RandomAffine, ToPILImage\n\n# These imports help us write the submission file\nimport json, csv\nimport os\nimport os.path\n\n# We will be using albumentations to perform data augmentation\nif (use_albumentations):\n    from albumentations import Compose\n    import albumentations.augmentations.transforms as transforms\n\n# This helps us keep a copy of model state dicts\nimport copy\n\n# To display random images\nfrom random import randrange\n\n# Enable hardware acceleration\ndevice = 'cpu'\nif torch.cuda.device_count() > 0 and torch.cuda.is_available():\n    print(\"Cuda installed! Running on GPU!\")\n    device = 'cuda'\nelse:\n    print(\"No GPU available!\")","execution_count":3,"outputs":[{"output_type":"stream","text":"Cuda installed! Running on GPU!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The means of our entire training set, calculated in another file:\nmeans = [0.4805, 0.4483, 0.3978]\nstds = [0.2177, 0.2138, 0.2136]","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_resnet_test = Compose([\n    Resize(299),\n    ToTensor(),\n    Normalize(means, stds)\n]) \n\ntransform_resnet_train = Compose([\n    #ToPILImage(),\n    RandomAffine(0, translate=(0.1, 0.1)),\n    RandomHorizontalFlip(),\n    RandomCrop(size=(64, 64), padding = 6),\n    Resize(224),\n    ToTensor(),\n    Normalize(mean=means, std=stds), \n]) \n","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageFolderWithPaths(ImageFolder):\n    \"\"\"Custom dataset that includes image file paths. Extends\n    torchvision.datasets.ImageFolder\n    \"\"\"\n\n    # override the __getitem__ method. this is the method that dataloader calls\n    def __getitem__(self, index):\n        # this is what ImageFolder normally returns \n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        # the image file path\n        path = self.imgs[index][0]\n        # make a new tuple that includes original and the path\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path\n\ndata_dir = \"../input/acse-miniproject/test/\"\ntest_set = ImageFolderWithPaths(data_dir, transform = transform_resnet_test) # our custom dataset\ntest_loader = DataLoader(test_set)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 42\nlr = 1e-2\nmomentum = 0.5\nbatch_size = 64\ntest_batch_size = 100\nn_epochs = 30","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data = ImageFolder(\"../input/acse-miniproject/train/\", transform = transform_resnet_test)\n\ntrain_size = int(0.95 * len(my_data))\nvalidation_size = len(my_data) - train_size\ntrain_dataset, validation_dataset = random_split(my_data, [train_size, validation_size])\n#train_dataset = my_data\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(validation_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trains our model on the training set\ndef train(model, optimizer, criterion, data_loader):\n    model.train()\n    train_loss, train_accuracy = 0, 0\n    for X, y in data_loader:\n        X, y = X.to(device), y.to(device)\n        optimizer.zero_grad()\n        a2, aux = model(X.view(-1, 3, 299, 299))\n        loss1 = criterion(a2, y)\n        loss2 = criterion(aux, y)\n        loss = loss1 + 0.4 * loss2\n        loss.backward()\n        train_loss += loss*X.size(0)\n        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n        optimizer.step()  \n        \n    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)\n  \n# Checks our model against the validation set\ndef validate(model, criterion, data_loader):\n    model.eval()\n    validation_loss, validation_accuracy = 0., 0.\n    for X, y in data_loader:\n        with torch.no_grad():\n            X, y = X.to(device), y.to(device)\n            a2= model(X.view(-1, 3, 299, 299))\n            loss = criterion(a2, y)\n            validation_loss += loss*X.size(0)\n            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n            \n    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)\n  \n# Evaluates our model's % accuracy\ndef evaluate(model, data_loader):\n    model.eval()\n    ys, y_preds = [], []\n    for X, y in data_loader:\n        with torch.no_grad():\n            X, y = X.to(device), y.to(device)\n            a2 = model(X.view(-1, 3, 299, 299))\n            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n            ys.append(y.cpu().numpy())\n            y_preds.append(y_pred.cpu().numpy())\n            \n    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)\n\n# Generates predictions in the required format\ndef predict(model, data_loader):\n    model.eval()\n    files, y_preds = [], []\n    for X, y, z in data_loader:\n        with torch.no_grad():\n            X, y = X.to(device), y.to(device)\n            a2 = model(X.view(-1, 3, 299, 299))\n            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n            y_preds.append(y_pred.cpu().numpy())\n            files.append(z)\n            \n    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n\n# Generates predictions by averaging two models\ndef multimodel_predict(model_first, model_second, data_loader):\n    model_first.eval()\n    model_second.eval()\n    files, y_preds = [], []\n    for X, y, z in data_loader:\n        with torch.no_grad():\n            X, y = X.to(device), y.to(device)\n            a2 = model_first(X.view(-1, 3, 224, 224))\n            a3 = model_second(X.view(-1, 3, 224, 224))\n            a2 = (a2 + a3) / 2.\n            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n            y_preds.append(y_pred.cpu().numpy())\n            files.append(z)\n            \n    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n\n# Generates predictions by averaging 3 models\ndef threemodel_predict(model_first, model_second, model_third, data_loader):\n    model_first.eval()\n    model_second.eval()\n    model_third.eval()\n    files, y_preds = [], []\n    for X, y, z in data_loader:\n        with torch.no_grad():\n            X, y = X.to(device), y.to(device)\n            a2 = model_first(X.view(-1, 3, 224, 224))\n            a3 = model_second(X.view(-1, 3, 224, 224))\n            a4 = model_third(X.view(-1, 3, 224, 224))\n            a2 = (a2 + a3 + a4) / 3.\n            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n            y_preds.append(y_pred.cpu().numpy())\n            files.append(z)\n            \n    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n\n# Checks the accuracy of our 3 model evaluation\ndef threemodel_evaluate(model_first, model_second, model_third, data_loader):\n    model_first.eval()\n    model_second.eval()\n    model_third.eval()\n    files, y_preds = [], []\n    for X, y in data_loader:\n        with torch.no_grad():\n            X, y = X.to(device), y.to(device)\n            a2 = model_first(X.view(-1, 3, 224, 224))\n            a3 = model_second(X.view(-1, 3, 224, 224))\n            a4 = model_third(X.view(-1, 3, 224, 224))\n            a2 = (a2 + a3 + a4) / 3.\n            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n            y_preds.append(y_pred.cpu().numpy())\n            \n    return np.concatenate(y_preds, 0)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''if (use_googlenet):\n    model = models.googlenet(pretrained=True).to(device)\nelif (use_resnet18):\n    model = models.resnet18(pretrained=True).to(device)\nelse:\n    model = models.densenet121(pretrained=True).to(device)\n\nnum_ftrs = model.classifier.in_features\nmodel.classifier = nn.Linear(num_ftrs, 200) # change number of output classes\n\n#num_ftrs = model.fc.in_features\n#model.fc = nn.Linear(num_ftrs, 200) # change number of output classes\n\n# uncomment to load previously created model:\n# model.load_state_dict(torch.load(\"./Googlenet_full_barely_augmented.pth\"))'''","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"'if (use_googlenet):\\n    model = models.googlenet(pretrained=True).to(device)\\nelif (use_resnet18):\\n    model = models.resnet18(pretrained=True).to(device)\\nelse:\\n    model = models.densenet121(pretrained=True).to(device)\\n\\nnum_ftrs = model.classifier.in_features\\nmodel.classifier = nn.Linear(num_ftrs, 200) # change number of output classes\\n\\n#num_ftrs = model.fc.in_features\\n#model.fc = nn.Linear(num_ftrs, 200) # change number of output classes\\n\\n# uncomment to load previously created model:\\n# model.load_state_dict(torch.load(\"./Googlenet_full_barely_augmented.pth\"))'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.inception_v3(pretrained=True, transform_input=True).to(device)\n\n#num_ftrs = model.classifier.in_features\n#model.classifier = nn.Linear(num_ftrs, 200) # change number of output classes\n\nnum_ftrs = model.AuxLogits.fc.in_features\nmodel.AuxLogits.fc = nn.Linear(num_ftrs, 200)\n\nprint(num_ftrs)\n\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 200) # change number of output classes","execution_count":11,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/checkpoints/inception_v3_google-1a9a5a14.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12ae35ad5d8344399c2b3179adb15810"}},"metadata":{}},{"output_type":"stream","text":"\n768\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (train_some_layers):\n    ct = 0\nelif (train_last_layer):\n    ct = -100\nelse:\n    ct = 12\n    \nfor child in model.children():\n    ct += 1\n    if ct < 12:\n        for param in child.parameters():\n            param.requires_grad = False\n    else:\n        for param in child.parameters():\n            param.requires_grad = True\n            \nprint(ct)\n\nif (train_last_layer):\n    for param in model.fc.parameters():\n        param.requires_grad = True","execution_count":12,"outputs":[{"output_type":"stream","text":"30\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\nliveloss = PlotLosses()\nfor epoch in range(40):\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    logs = {}\n    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n\n    logs['' + 'log loss'] = train_loss.item()\n    logs['' + 'accuracy'] = train_accuracy.item()\n    \n    validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n    logs['val_' + 'log loss'] = validation_loss.item()\n    logs['val_' + 'accuracy'] = validation_accuracy.item()\n    \n    if validation_accuracy.item() > best_acc:\n        best_acc = validation_accuracy.item()\n        best_model_wts = copy.deepcopy(model.state_dict())\n    \n    liveloss.update(logs)\n    liveloss.draw()","execution_count":16,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"too many values to unpack (expected 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-94b23957195f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mvalidation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'log loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-9350e890e92c>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, criterion, data_loader)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":21,"outputs":[{"output_type":"stream","text":"0.5992\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}