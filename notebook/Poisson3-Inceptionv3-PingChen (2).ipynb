{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install pycm livelossplot albumentations\n%pylab inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"use_albumentations = False\n\ntrain_whole_model = False\ntrain_last_layer = True\ntrain_some_layers = False\n\nuse_googlenet = False\nuse_resnet50 = False\nuse_inceptionV3 = True \nuse_resnet18 = False\n\n# Auxliary output for inceptionV3\nAuxliary_output = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score # this allows us to evaluate our model at every iteration\nfrom sklearn.metrics import f1_score # this allows us to evaluate our validation accuracy\nfrom sklearn.model_selection import StratifiedShuffleSplit # this allows us to create a random validation split\n\n# These imports help plot the convergence and create the confusion matrix\nfrom livelossplot import PlotLosses\nfrom pycm import *\n\n# These imports help us create models and datasets\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\n\n# This allows me to create my own custom dataset\nfrom torch.utils.data import Dataset \nfrom torchvision.datasets.folder import *\n\n# This allows me to import pretrained models for transfer learning\nimport torchvision.models as models\n\n# This allows me to do a number of transforms for data augmentation later on\nfrom torchvision.transforms import Compose, ToTensor, ColorJitter, Resize, Normalize, RandomApply, RandomChoice, RandomRotation, RandomCrop, RandomResizedCrop, RandomHorizontalFlip, RandomAffine, ToPILImage\n\n# These imports help us write the submission file\nimport json, csv\nimport os\nimport os.path\n\n# We will be using albumentations to perform data augmentation\nif (use_albumentations):\n    from albumentations import Compose\n    import albumentations.augmentations.transforms as transforms\n\n# This helps us keep a copy of model state dicts\nimport copy\n\n# To display random images\nfrom random import randrange\n\n# Enable hardware acceleration\ndevice = 'cpu'\nif torch.cuda.device_count() > 0 and torch.cuda.is_available():\n    print(\"Cuda installed! Running on GPU!\")\n    device = 'cuda'\nelse:\n    print(\"No GPU available!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The means of our entire training set, calculated in another file:\nmeans = [0.4805, 0.4483, 0.3978]\nstds = [0.2177, 0.2138, 0.2136]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rewritten to help separate data from labels\ndef my_make_dataset(directory, class_to_idx, extensions=None, is_valid_file=None):\n    data = []\n    targets = []\n    directory = os.path.expanduser(directory)\n    both_none = extensions is None and is_valid_file is None\n    both_something = extensions is not None and is_valid_file is not None\n    if both_none or both_something:\n        raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n    if extensions is not None:\n        def is_valid_file(x):\n            return has_file_allowed_extension(x, extensions)\n    for target_class in sorted(class_to_idx.keys()):\n        class_index = class_to_idx[target_class]\n        target_dir = os.path.join(directory, target_class)\n        if not os.path.isdir(target_dir):\n            continue\n        for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n            for fname in sorted(fnames):\n                path = os.path.join(root, fname)\n                if is_valid_file(path):\n                    item = path, class_index\n                    data.append(path)\n                    targets.append(class_index)\n    return data, targets\n\n\n# A custom dataset class that will work with albumentations because ImageFolder does not\nclass AlbumentationImageFolder(ImageFolder):\n    def __init__(self, root, extensions=IMG_EXTENSIONS, transform=None,\n                 target_transform=None, is_valid_file=None, augmentation=None):\n        super(ImageFolder, self).__init__(root, default_loader, IMG_EXTENSIONS if is_valid_file is None else None,\n                                          transform=transform,\n                                          target_transform=target_transform,\n                                          is_valid_file=is_valid_file)\n        \n        classes, class_to_idx = self._find_classes(self.root)\n        data, targets = my_make_dataset(self.root, class_to_idx, extensions, is_valid_file)\n        self.imgs = self.samples\n        self.data = [ToTensor()(self.loader(data[i])) for i in range(len(data))]\n        self.targets = targets\n        self.augmentation = augmentation\n\n    def __getitem__(self, idx):\n        path, target = self.samples[idx]\n        sample = self.loader(path)\n        if self.augmentation is not None:\n            sample = self.augmentation(image=np.asarray(sample))['image']\n            sample = np.transpose(sample, (2, 0, 1)).astype(np.float32)\n            sample = torch.tensor(sample, dtype=torch.float)\n            \n        return sample, target\n    \nclass CustomImageTensorDatasetNA(Dataset):\n    def __init__(self, data, targets, transform=None):\n        \"\"\"\n        Args:\n            data (Tensor): A tensor containing the data e.g. images\n            targets (Tensor): A tensor containing all the labels\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.data = data\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample, label = self.data[idx], self.targets[idx]\n        if self.transform:\n            sample = self.transform(sample)\n            \n        return sample, label\n    \nclass CustomImageTensorDataset(Dataset):\n    def __init__(self, data, targets, transform=None):\n        \"\"\"\n        Args:\n            data (Tensor): A tensor containing the data e.g. images\n            targets (Tensor): A tensor containing all the labels\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.data = data\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample, label = self.data[idx], self.targets[idx]\n        if self.transform:\n            sample = ToPILImage()(sample).convert(\"RGB\")\n            sample = self.transform(image=np.array(sample, dtype = np.uint8))['image']\n            sample = np.transpose(sample, (2, 0, 1)).astype(np.float32)\n            sample = torch.tensor(sample, dtype=torch.float)\n\n        return sample, label\n    \n# A custom dataset class that retains filenames for use in creating the csv file\nclass ImageFolderWithPaths(ImageFolder):\n    \"\"\"Custom dataset that includes image file paths. Extends\n    torchvision.datasets.ImageFolder\n    \"\"\"\n\n    # We only need to override the __getitem__ method\n    def __getitem__(self, index):\n        # this is what ImageFolder normally returns \n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        # the image file path\n        path = self.imgs[index][0]\n        # make a new tuple that includes original and the path\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path\n    \nfrom torchvision.datasets.folder import *\nclass AlbumentationImageFolder(ImageFolder):\n    '''Custom ImageFolder to implement albumentations transforms as well as pytorch transforms'''\n    \n    def __init__(self, root, extensions=IMG_EXTENSIONS, transform=None,\n                 target_transform=None, is_valid_file=None, augmentation=None):\n        super(ImageFolder, self).__init__(root, default_loader, IMG_EXTENSIONS if is_valid_file is None else None,\n                                          transform=transform,\n                                          target_transform=target_transform,\n                                          is_valid_file=is_valid_file)\n        self.imgs = self.samples\n        self.augmentation = augmentation\n    def __getitem__(self, idx):\n        '''Returns a tuple of the image tensor and the label'''\n        path, target = self.samples[idx]\n        sample = np.asarray(self.loader(path))\n        if self.augmentation is not None:\n            sample = self.augmentation(image=sample)['image']\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        sample = np.transpose(sample, (2, 0, 1)).astype(np.float32)\n        sample = torch.tensor(sample, dtype=torch.float)\n            \n        return sample, target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transform = Compose([\n#     ToPILImage(),\n#     RandomApply([RandomChoice([RandomCrop(size=[64, 64], padding=4), RandomAffine(0, translate=(0, 0))])]),\n    RandomHorizontalFlip(),\n#     ColorJitter(brightness=0.1, contrast=0.05, saturation=2, hue=0.08),\n    Resize(299),\n    ToTensor(),\n    Normalize(mean=means, std=stds), \n])\n\nvalidation_test_transform = Compose([\n#     ToPILImage(),\n    Resize(299),\n    ToTensor(),\n    Normalize(mean=means, std=stds)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data = ImageFolder(\"../input/acse-miniproject/train/\", transform = train_transform )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test = ImageFolderWithPaths(\"../input/acse-miniproject/test/\", transform = validation_test_transform) # our custom dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 42\nlr = 1e-3\nmomentum = 0.8\nbatch_size = 64\ntest_batch_size = 100\nn_epochs = 6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shuffleTorch = torch.zeros(100000, 3, 64, 64)\n# shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1).split(shuffleTorch, my_data.targets)\n# indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]\n# Split validation and training\ntrain_size = int(0.9 * len(my_data))\nvalidation_size = len(my_data) - train_size\ntrain_dataset, validation_dataset = random_split(my_data, [train_size, validation_size])\n# DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(validation_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n# Need it if we need output file\ntest_loader = DataLoader(data_test, batch_size=test_batch_size, shuffle=False, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# my_data.data = torch.stack(my_data.data)\n\n# X_train, y_train = my_data.data[indices[0]].float(), torch.from_numpy(np.array(my_data.targets)[indices[0]])\n# X_val, y_val = my_data.data[indices[1]].float(),  torch.from_numpy(np.array(my_data.targets)[indices[1]])\n# X_test, y_test = test_data.float(),  torch.from_numpy(np.array(test_data.targets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if (use_albumentations):\n#     data_train = CustomImageTensorDataset(X_train, y_train.long(), transform=Compose([\n#                         transforms.RandomContrast(),\n#                         transforms.HorizontalFlip(),\n#                         transforms.HueSaturationValue(),\n#                         transforms.Resize(224, 224),\n#                         transforms.Normalize(means, stds)\n#                         ]))\n#     data_validate = CustomImageTensorDataset(X_val, y_val.long(), transform=Compose([transforms.Resize(224, 224),\n#                         transforms.Normalize(means, stds)]))\n\n# else:\n#     data_train = CustomImageTensorDatasetNA(X_train, y_train.long(), transform=train_transform)\n#     data_validate = CustomImageTensorDatasetNA(X_val, y_val.long(), transform=validation_test_transform)\n\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n# validation_loader = DataLoader(validate_data, batch_size=test_batch_size, shuffle=False, num_workers=0)\n# # Need it if we need output file\n# test_loader = DataLoader(data_test, batch_size=test_batch_size, shuffle=False, num_workers=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, optimizer, criterion, data_loader):\n    model.train()\n    train_loss, train_accuracy = 0, 0\n    for X, y in data_loader:\n        X, y = X.to(device), y.to(device)\n        optimizer.zero_grad()\n        a2, aux = model(X.view(-1, 3, 299, 299))\n        loss1 = criterion(a2, y)\n        loss2 = criterion(aux, y)\n        loss = loss1 + 0.4 * loss2\n        loss.backward()\n        train_loss += loss*X.size(0)\n        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n        optimizer.step()  \n        \n    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)\n  \ndef validate(model, criterion, data_loader):\n    model.eval()\n    validation_loss, validation_accuracy = 0., 0.\n    for X, y in data_loader:\n        with torch.no_grad():\n            X, y = X.to(device), y.to(device)\n            a2 = model(X.view(-1, 3, 299, 299))\n            loss = criterion(a2, y)\n            validation_loss += loss*X.size(0)\n            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n            \n    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)\n  \n# Evaluates our model's % accuracy\ndef evaluate(model, data_loader):\n    model.eval()\n    ys, y_preds = [], []\n    for X, y in data_loader:\n        with torch.no_grad():\n            X, y = X.to(device), y.to(device)\n            a2 = model(X.view(-1, 3, 299, 299))\n            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n            ys.append(y.cpu().numpy())\n            y_preds.append(y_pred.cpu().numpy())\n            \n    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)\n# Generates predictions in the required format\ndef predict(model, data_loader):\n    model.eval()\n    files, y_preds = [], []\n    for X, y, z in data_loader:\n        with torch.no_grad():\n            X, y = X.to(device), y.to(device)\n            a2 = model(X.view(-1, 3, 299, 299))\n            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n            y_preds.append(y_pred.cpu().numpy())\n            files.append(z)\n            \n    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n# ===================================================================================\n# The below code is for input 224 might not suit for Inception v3\n\n# Generates predictions by averaging two models\ndef multimodel_predict(model_first, model_second, data_loader):\n    model_first.eval()\n    model_second.eval()\n    files, y_preds = [], []\n    for X, y, z in data_loader:\n        with torch.no_grad():\n            X, y = X.to(device), y.to(device)\n            a2 = model_first(X.view(-1, 3, 224, 224))\n            a3 = model_second(X.view(-1, 3, 224, 224))\n            a2 = (a2 + a3) / 2.\n            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n            y_preds.append(y_pred.cpu().numpy())\n            files.append(z)\n            \n    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n\n# Generates predictions by averaging 3 models\ndef threemodel_predict(model_first, model_second, model_third, data_loader):\n    model_first.eval()\n    model_second.eval()\n    model_third.eval()\n    files, y_preds = [], []\n    for X, y, z in data_loader:\n        with torch.no_grad():\n            X, y = X.to(device), y.to(device)\n            a2 = model_first(X.view(-1, 3, 224, 224))\n            a3 = model_second(X.view(-1, 3, 224, 224))\n            a4 = model_third(X.view(-1, 3, 224, 224))\n            a2 = (a2 + a3 + a4) / 3.\n            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n            y_preds.append(y_pred.cpu().numpy())\n            files.append(z)\n            \n    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n\n# Checks the accuracy of our 3 model evaluation\ndef threemodel_evaluate(model_first, model_second, model_third, data_loader):\n    model_first.eval()\n    model_second.eval()\n    model_third.eval()\n    files, y_preds = [], []\n    for X, y in data_loader:\n        with torch.no_grad():\n            X, y = X.to(device), y.to(device)\n            a2 = model_first(X.view(-1, 3, 224, 224))\n            a3 = model_second(X.view(-1, 3, 224, 224))\n            a4 = model_third(X.view(-1, 3, 224, 224))\n            a2 = (a2 + a3 + a4) / 3.\n            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n            y_preds.append(y_pred.cpu().numpy())\n            \n    return np.concatenate(y_preds, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (use_googlenet):\n    model = models.googlenet(pretrained=True).to(device)\nelif (use_resnet18):\n    model = models.resnet18(pretrained=True).to(device)\nelif (use_inceptionV3):\n    model = models.inception_v3(pretrained=True).to(device)\n    \n    # Auxliary output for InceptionV3\n    if Auxliary_output:\n        # Handle the auxilary net\n        num_ftrs = model.AuxLogits.fc.in_features\n        model.AuxLogits.fc = nn.Linear(num_ftrs, 200)\n        # Handle the primary net\n        num_ftrs = model.fc.in_features\n        model.fc = nn.Linear(num_ftrs,200)\n    else:\n        model.aux_logits=False\nelse:\n    model = models.resnet50(pretrained=True).to(device)\n\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 200) # change number of output classes\n\n# uncomment to load previously created model:\n# model.load_state_dict(torch.load(\"./Googlenet_full_barely_augmented.pth\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \nif (train_some_layers):\n    ct = 0\nelif (train_last_layer):\n    ct = -100\nelse:\n#     This number decides first ct number of \n    ct = 12\n    \nfor child in model.children():\n    ct += 1\n    if ct < 12:\n#         freeze the preivous 11 layers\n        for param in child.parameters():\n            param.requires_grad = False\n    else:\n        for param in child.parameters():\n            param.requires_grad = True\n            \nprint(ct)\n\nif (train_last_layer):\n    for param in model.fc.parameters():\n        param.requires_grad = True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\nliveloss = PlotLosses()\n\nfor epoch in range(n_epochs):\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    logs = {}\n    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n\n    logs['' + 'log loss'] = train_loss.item()\n    logs['' + 'accuracy'] = train_accuracy.item()\n    \n    validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n    logs['val_' + 'log loss'] = validation_loss.item()\n    logs['val_' + 'accuracy'] = validation_accuracy.item()\n    \n    if validation_accuracy.item() > best_acc:\n        best_acc = validation_accuracy.item()\n        best_model_wts = copy.deepcopy(model.state_dict())\n    \n    liveloss.update(logs)\n    liveloss.draw()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred, y_true = evaluate(model, validation_loader)\n\nf1 = f1_score(y_true, y_pred, average=\"macro\")\nprint(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_predictions, my_files = predict(model, test_loader)\n\nmy_files_clean = [my_files[i][14:] for i in range(len(my_files))]\n\nwith open('Inceptionv3_mom0_9.csv', 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n    wr = csv.writer(myfile)\n    wr.writerow((\"Filename\", \"Label\"))\n    wr.writerows(zip(my_files_clean,my_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change your model name:\nmodel_save_name = 'Inceptionv3_1.pth'\npath = F\"./{model_save_name}\" \ntorch.save(model.state_dict(), path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Trial 1\n    -googlenet + resnet50\n    -Transform:\n        RandomApply([RandomChoice([RandomCrop(size=[64, 64], padding=4), RandomAffine(0, translate=(0.1, 0.1))])]),\n        RandomHorizontalFlip(),\n        ColorJitter(brightness=0.1, contrast=0.05, saturation=2, hue=0.08),\n    -Train on all dataset\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}