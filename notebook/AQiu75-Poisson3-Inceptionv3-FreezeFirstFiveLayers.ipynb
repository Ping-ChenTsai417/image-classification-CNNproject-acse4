{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycm in /opt/firedrake/lib/python3.6/site-packages (2.6)\n",
      "Requirement already satisfied: livelossplot in /opt/firedrake/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: albumentations in /opt/firedrake/lib/python3.6/site-packages (0.4.5)\n",
      "Requirement already satisfied: art>=1.8 in /opt/firedrake/lib/python3.6/site-packages (from pycm) (4.5)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/firedrake/lib/python3.6/site-packages (from pycm) (1.18.2)\n",
      "Requirement already satisfied: matplotlib; python_version >= \"3.6\" in /opt/firedrake/lib/python3.6/site-packages (from livelossplot) (3.2.1)\n",
      "Requirement already satisfied: ipython in /opt/firedrake/lib/python3.6/site-packages (from livelossplot) (7.13.0)\n",
      "Requirement already satisfied: imgaug<0.2.7,>=0.2.5 in /opt/firedrake/lib/python3.6/site-packages (from albumentations) (0.2.6)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/firedrake/lib/python3.6/site-packages (from albumentations) (4.2.0.34)\n",
      "Requirement already satisfied: scipy in /opt/firedrake/lib/python3.6/site-packages (from albumentations) (1.4.1)\n",
      "Requirement already satisfied: PyYAML in /opt/firedrake/lib/python3.6/site-packages (from albumentations) (5.3.1)\n",
      "Requirement already satisfied: coverage>=4.1 in /opt/firedrake/lib/python3.6/site-packages (from art>=1.8->pycm) (4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/firedrake/lib/python3.6/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/firedrake/lib/python3.6/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/firedrake/lib/python3.6/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/firedrake/lib/python3.6/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.4.6)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (0.16.0)\n",
      "Requirement already satisfied: pygments in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (2.6.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (4.3.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (3.0.5)\n",
      "Requirement already satisfied: decorator in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (4.4.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (46.1.3)\n",
      "Requirement already satisfied: backcall in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (0.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (0.7.5)\n",
      "Requirement already satisfied: six in /opt/firedrake/lib/python3.6/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.14.0)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /opt/firedrake/lib/python3.6/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n",
      "Requirement already satisfied: parso>=0.5.2 in /opt/firedrake/lib/python3.6/site-packages (from jedi>=0.10->ipython->livelossplot) (0.6.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/firedrake/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython->livelossplot) (0.6.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/firedrake/lib/python3.6/site-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /opt/firedrake/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->livelossplot) (0.1.9)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/firedrake/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/firedrake/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /opt/firedrake/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.0.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/firedrake/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/opt/firedrake/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "!pip install pycm livelossplot albumentations\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "use_albumentations = False\n",
    "\n",
    "train_whole_model = False\n",
    "train_last_layer = False\n",
    "train_some_layers = True\n",
    "\n",
    "use_googlenet = False\n",
    "use_resnet50 = False\n",
    "use_inceptionV3 = True \n",
    "use_resnet18 = False\n",
    "\n",
    "# Auxliary output for inceptionV3\n",
    "Auxliary_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda installed! Running on GPU!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score # this allows us to evaluate our model at every iteration\n",
    "from sklearn.metrics import f1_score # this allows us to evaluate our validation accuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit # this allows us to create a random validation split\n",
    "\n",
    "# These imports help plot the convergence and create the confusion matrix\n",
    "from livelossplot import PlotLosses\n",
    "from pycm import *\n",
    "\n",
    "# These imports help us create models and datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# This allows me to create my own custom dataset\n",
    "from torch.utils.data import Dataset \n",
    "from torchvision.datasets.folder import *\n",
    "\n",
    "# This allows me to import pretrained models for transfer learning\n",
    "import torchvision.models as models\n",
    "\n",
    "# This allows me to do a number of transforms for data augmentation later on\n",
    "from torchvision.transforms import Compose, ToTensor, ColorJitter, Resize, Normalize, RandomApply, RandomChoice, RandomRotation, RandomCrop, RandomResizedCrop, RandomHorizontalFlip, RandomAffine, ToPILImage\n",
    "\n",
    "# These imports help us write the submission file\n",
    "import json, csv\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# We will be using albumentations to perform data augmentation\n",
    "if (use_albumentations):\n",
    "    from albumentations import Compose\n",
    "    import albumentations.augmentations.transforms as transforms\n",
    "\n",
    "# This helps us keep a copy of model state dicts\n",
    "import copy\n",
    "\n",
    "# To display random images\n",
    "from random import randrange\n",
    "\n",
    "# Enable hardware acceleration\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda:1'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The means of our entire training set, calculated in another file:\n",
    "means = [0.4805, 0.4483, 0.3978]\n",
    "stds = [0.2177, 0.2138, 0.2136]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewritten to help separate data from labels\n",
    "def my_make_dataset(directory, class_to_idx, extensions=None, is_valid_file=None):\n",
    "    data = []\n",
    "    targets = []\n",
    "    directory = os.path.expanduser(directory)\n",
    "    both_none = extensions is None and is_valid_file is None\n",
    "    both_something = extensions is not None and is_valid_file is not None\n",
    "    if both_none or both_something:\n",
    "        raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
    "    if extensions is not None:\n",
    "        def is_valid_file(x):\n",
    "            return has_file_allowed_extension(x, extensions)\n",
    "    for target_class in sorted(class_to_idx.keys()):\n",
    "        class_index = class_to_idx[target_class]\n",
    "        target_dir = os.path.join(directory, target_class)\n",
    "        if not os.path.isdir(target_dir):\n",
    "            continue\n",
    "        for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
    "            for fname in sorted(fnames):\n",
    "                path = os.path.join(root, fname)\n",
    "                if is_valid_file(path):\n",
    "                    item = path, class_index\n",
    "                    data.append(path)\n",
    "                    targets.append(class_index)\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "# A custom dataset class that will work with albumentations because ImageFolder does not\n",
    "class AlbumentationImageFolder(ImageFolder):\n",
    "    def __init__(self, root, extensions=IMG_EXTENSIONS, transform=None,\n",
    "                 target_transform=None, is_valid_file=None, augmentation=None):\n",
    "        super(ImageFolder, self).__init__(root, default_loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
    "                                          transform=transform,\n",
    "                                          target_transform=target_transform,\n",
    "                                          is_valid_file=is_valid_file)\n",
    "        \n",
    "        classes, class_to_idx = self._find_classes(self.root)\n",
    "        data, targets = my_make_dataset(self.root, class_to_idx, extensions, is_valid_file)\n",
    "        self.imgs = self.samples\n",
    "        self.data = [ToTensor()(self.loader(data[i])) for i in range(len(data))]\n",
    "        self.targets = targets\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        sample = self.loader(path)\n",
    "        if self.augmentation is not None:\n",
    "            sample = self.augmentation(image=np.asarray(sample))['image']\n",
    "            sample = np.transpose(sample, (2, 0, 1)).astype(np.float32)\n",
    "            sample = torch.tensor(sample, dtype=torch.float)\n",
    "            \n",
    "        return sample, target\n",
    "    \n",
    "class CustomImageTensorDatasetNA(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Tensor): A tensor containing the data e.g. images\n",
    "            targets (Tensor): A tensor containing all the labels\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, label = self.data[idx], self.targets[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample, label\n",
    "    \n",
    "class CustomImageTensorDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Tensor): A tensor containing the data e.g. images\n",
    "            targets (Tensor): A tensor containing all the labels\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, label = self.data[idx], self.targets[idx]\n",
    "        if self.transform:\n",
    "            sample = ToPILImage()(sample).convert(\"RGB\")\n",
    "            sample = self.transform(image=np.array(sample, dtype = np.uint8))['image']\n",
    "            sample = np.transpose(sample, (2, 0, 1)).astype(np.float32)\n",
    "            sample = torch.tensor(sample, dtype=torch.float)\n",
    "\n",
    "        return sample, label\n",
    "    \n",
    "# A custom dataset class that retains filenames for use in creating the csv file\n",
    "class ImageFolderWithPaths(ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # We only need to override the __getitem__ method\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "    \n",
    "from torchvision.datasets.folder import *\n",
    "class AlbumentationImageFolder(ImageFolder):\n",
    "    '''Custom ImageFolder to implement albumentations transforms as well as pytorch transforms'''\n",
    "    \n",
    "    def __init__(self, root, extensions=IMG_EXTENSIONS, transform=None,\n",
    "                 target_transform=None, is_valid_file=None, augmentation=None):\n",
    "        super(ImageFolder, self).__init__(root, default_loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
    "                                          transform=transform,\n",
    "                                          target_transform=target_transform,\n",
    "                                          is_valid_file=is_valid_file)\n",
    "        self.imgs = self.samples\n",
    "        self.augmentation = augmentation\n",
    "    def __getitem__(self, idx):\n",
    "        '''Returns a tuple of the image tensor and the label'''\n",
    "        path, target = self.samples[idx]\n",
    "        sample = np.asarray(self.loader(path))\n",
    "        if self.augmentation is not None:\n",
    "            sample = self.augmentation(image=sample)['image']\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        sample = np.transpose(sample, (2, 0, 1)).astype(np.float32)\n",
    "        sample = torch.tensor(sample, dtype=torch.float)\n",
    "            \n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "#     ToPILImage(),\n",
    "#     RandomApply([RandomChoice([RandomCrop(size=[64, 64], padding=4), RandomAffine(0, translate=(0, 0))])]),\n",
    "    RandomHorizontalFlip(),\n",
    "#     ColorJitter(brightness=0.1, contrast=0.05, saturation=2, hue=0.08),\n",
    "    Resize(299),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=means, std=stds), \n",
    "])\n",
    "\n",
    "validation_test_transform = Compose([\n",
    "#     ToPILImage(),\n",
    "    Resize(299),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=means, std=stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = ImageFolder(\"./train/\", transform = train_transform )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = ImageFolderWithPaths(\"./test/\", transform = validation_test_transform) # our custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "lr = 1e-3\n",
    "momentum = 0.5\n",
    "batch_size = 64\n",
    "test_batch_size = 100\n",
    "n_epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffleTorch = torch.zeros(100000, 3, 64, 64)\n",
    "# shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1).split(shuffleTorch, my_data.targets)\n",
    "# indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]\n",
    "# Split validation and training\n",
    "train_size = int(0.9 * len(my_data))\n",
    "validation_size = len(my_data) - train_size\n",
    "train_dataset, validation_dataset = random_split(my_data, [train_size, validation_size])\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "# Need it if we need output file\n",
    "test_loader = DataLoader(data_test, batch_size=test_batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_data.data = torch.stack(my_data.data)\n",
    "\n",
    "# X_train, y_train = my_data.data[indices[0]].float(), torch.from_numpy(np.array(my_data.targets)[indices[0]])\n",
    "# X_val, y_val = my_data.data[indices[1]].float(),  torch.from_numpy(np.array(my_data.targets)[indices[1]])\n",
    "# X_test, y_test = test_data.float(),  torch.from_numpy(np.array(test_data.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if (use_albumentations):\n",
    "#     data_train = CustomImageTensorDataset(X_train, y_train.long(), transform=Compose([\n",
    "#                         transforms.RandomContrast(),\n",
    "#                         transforms.HorizontalFlip(),\n",
    "#                         transforms.HueSaturationValue(),\n",
    "#                         transforms.Resize(224, 224),\n",
    "#                         transforms.Normalize(means, stds)\n",
    "#                         ]))\n",
    "#     data_validate = CustomImageTensorDataset(X_val, y_val.long(), transform=Compose([transforms.Resize(224, 224),\n",
    "#                         transforms.Normalize(means, stds)]))\n",
    "\n",
    "# else:\n",
    "#     data_train = CustomImageTensorDatasetNA(X_train, y_train.long(), transform=train_transform)\n",
    "#     data_validate = CustomImageTensorDatasetNA(X_val, y_val.long(), transform=validation_test_transform)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# validation_loader = DataLoader(validate_data, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "# # Need it if we need output file\n",
    "# test_loader = DataLoader(data_test, batch_size=test_batch_size, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_loader):\n",
    "    model.train()\n",
    "    train_loss, train_accuracy = 0, 0\n",
    "    for X, y in data_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        a2, aux = model(X.view(-1, 3, 299, 299))\n",
    "        loss1 = criterion(a2, y)\n",
    "        loss2 = criterion(aux, y)\n",
    "        loss = loss1 + 0.4 * loss2\n",
    "        loss.backward()\n",
    "        train_loss += loss*X.size(0)\n",
    "        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n",
    "        optimizer.step()  \n",
    "        \n",
    "    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)\n",
    "  \n",
    "def validate(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    validation_loss, validation_accuracy = 0., 0.\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 3, 299, 299))\n",
    "            loss = criterion(a2, y)\n",
    "            validation_loss += loss*X.size(0)\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n",
    "            \n",
    "    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)\n",
    "  \n",
    "# Evaluates our model's % accuracy\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    ys, y_preds = [], []\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 3, 299, 299))\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            ys.append(y.cpu().numpy())\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)\n",
    "# Generates predictions in the required format\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    files, y_preds = [], []\n",
    "    for X, y, z in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 3, 299, 299))\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            files.append(z)\n",
    "            \n",
    "    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n",
    "# ===================================================================================\n",
    "# The below code is for input 224 might not suit for Inception v3\n",
    "\n",
    "# Generates predictions by averaging two models\n",
    "def multimodel_predict(model_first, model_second, data_loader):\n",
    "    model_first.eval()\n",
    "    model_second.eval()\n",
    "    files, y_preds = [], []\n",
    "    for X, y, z in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model_first(X.view(-1, 3, 224, 224))\n",
    "            a3 = model_second(X.view(-1, 3, 224, 224))\n",
    "            a2 = (a2 + a3) / 2.\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            files.append(z)\n",
    "            \n",
    "    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n",
    "\n",
    "# Generates predictions by averaging 3 models\n",
    "def threemodel_predict(model_first, model_second, model_third, data_loader):\n",
    "    model_first.eval()\n",
    "    model_second.eval()\n",
    "    model_third.eval()\n",
    "    files, y_preds = [], []\n",
    "    for X, y, z in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model_first(X.view(-1, 3, 224, 224))\n",
    "            a3 = model_second(X.view(-1, 3, 224, 224))\n",
    "            a4 = model_third(X.view(-1, 3, 224, 224))\n",
    "            a2 = (a2 + a3 + a4) / 3.\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            files.append(z)\n",
    "            \n",
    "    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n",
    "\n",
    "# Checks the accuracy of our 3 model evaluation\n",
    "def threemodel_evaluate(model_first, model_second, model_third, data_loader):\n",
    "    model_first.eval()\n",
    "    model_second.eval()\n",
    "    model_third.eval()\n",
    "    files, y_preds = [], []\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model_first(X.view(-1, 3, 224, 224))\n",
    "            a3 = model_second(X.view(-1, 3, 224, 224))\n",
    "            a4 = model_third(X.view(-1, 3, 224, 224))\n",
    "            a2 = (a2 + a3 + a4) / 3.\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(y_preds, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_googlenet):\n",
    "    model = models.googlenet(pretrained=True).to(device)\n",
    "elif (use_resnet18):\n",
    "    model = models.resnet18(pretrained=True).to(device)\n",
    "elif (use_inceptionV3):\n",
    "    model = models.inception_v3(pretrained=True).to(device)\n",
    "    \n",
    "    # Auxliary output for InceptionV3\n",
    "    if Auxliary_output:\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model.AuxLogits.fc.in_features\n",
    "        model.AuxLogits.fc = nn.Linear(num_ftrs, 200)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs,200)\n",
    "    else:\n",
    "        model.aux_logits=False\n",
    "else:\n",
    "    model = models.resnet50(pretrained=True).to(device)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 200) # change number of output classes\n",
    "\n",
    "# uncomment to load previously created model:\n",
    "# model.load_state_dict(torch.load(\"./Googlenet_full_barely_augmented.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "if (train_some_layers):\n",
    "    ct = 0\n",
    "elif (train_last_layer):\n",
    "    ct = -100\n",
    "else:\n",
    "#     This number decides first ct number of \n",
    "    ct = 8\n",
    "    \n",
    "for child in model.children():\n",
    "    ct += 1\n",
    "    if ct < 8:\n",
    "#         freeze the preivous 11 layers\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "print(ct)\n",
    "\n",
    "if (train_last_layer):\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5, weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    logs = {}\n",
    "    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "    logs['' + 'log loss'] = train_loss.item()\n",
    "    logs['' + 'accuracy'] = train_accuracy.item()\n",
    "    \n",
    "    validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n",
    "    logs['val_' + 'log loss'] = validation_loss.item()\n",
    "    logs['val_' + 'accuracy'] = validation_accuracy.item()\n",
    "    \n",
    "    if validation_accuracy.item() > best_acc:\n",
    "        best_acc = validation_accuracy.item()\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = evaluate(model, validation_loader)\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictions, my_files = predict(model, test_loader)\n",
    "\n",
    "my_files_clean = [my_files[i][14:] for i in range(len(my_files))]\n",
    "\n",
    "with open('Inceptionv3_mom0_9.csv', 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow((\"Filename\", \"Label\"))\n",
    "    wr.writerows(zip(my_files_clean,my_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change your model name:\n",
    "model_save_name = 'Inceptionv3_1.pth'\n",
    "path = F\"./{model_save_name}\" \n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trial 1\n",
    "    -googlenet + resnet50\n",
    "    -Transform:\n",
    "        RandomApply([RandomChoice([RandomCrop(size=[64, 64], padding=4), RandomAffine(0, translate=(0.1, 0.1))])]),\n",
    "        RandomHorizontalFlip(),\n",
    "        ColorJitter(brightness=0.1, contrast=0.05, saturation=2, hue=0.08),\n",
    "    -Train on all dataset\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
