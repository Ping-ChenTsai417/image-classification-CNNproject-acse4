{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This cell will help us plot the convergence of the model in real time\n!pip install pycm livelossplot\n%pylab inline","execution_count":30,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: pycm in /opt/conda/lib/python3.7/site-packages (2.7)\nRequirement already satisfied: livelossplot in /opt/conda/lib/python3.7/site-packages (0.5.0)\nRequirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from pycm) (1.18.1)\nRequirement already satisfied: art>=1.8 in /opt/conda/lib/python3.7/site-packages (from pycm) (4.6)\nRequirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from livelossplot) (7.13.0)\nRequirement already satisfied: matplotlib; python_version >= \"3.6\" in /opt/conda/lib/python3.7/site-packages (from livelossplot) (3.2.1)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (0.1.0)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (3.0.5)\nRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (46.1.3.post20200325)\nRequirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (4.8.0)\nRequirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (4.3.3)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (4.4.2)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (2.6.1)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (0.7.5)\nRequirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython->livelossplot) (0.15.2)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.8.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (1.2.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (0.10.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->livelossplot) (0.1.9)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->livelossplot) (0.6.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython->livelossplot) (1.14.0)\nRequirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\nRequirement already satisfied: parso>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython->livelossplot) (0.5.2)\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nPopulating the interactive namespace from numpy and matplotlib\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# This cell performs all the imports that we will need in this coursework\nfrom sklearn.metrics import accuracy_score # this allows us to evaluate our model at every iteration\nfrom sklearn.metrics import f1_score # this allows us to evaluate our validation accuracy\nfrom sklearn.model_selection import StratifiedShuffleSplit # this allows us to create a random validation split\n\n# These imports help plot the convergence and create the confusion matrix\nfrom livelossplot import PlotLosses\nfrom pycm import *\n\n# These imports help us create models and datasets\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\n\n# This allows me to create my own custom dataset\nfrom torch.utils.data import Dataset \n\n# This allows me to import pretrained models for transfer learning\nimport torchvision.models as torchmodels\n\n# This allows me to do a number of transforms for data augmentation later on\nfrom torchvision.transforms import Compose, ToTensor, Resize, Normalize, RandomApply, RandomChoice, RandomRotation, RandomCrop, RandomHorizontalFlip, RandomAffine, ToPILImage\n\n# These imports help us write the submission file\nimport json, csv\n","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This functions sets up all our random seeds so that the results will be reproducible\ndef set_seed(seed):\n    \"\"\"\n    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n    torch.backends.cudnn.enabled   = False\n\n    return True\n\n# Enable hardware acceleration\ndevice = 'cpu'\nif torch.cuda.device_count() > 0 and torch.cuda.is_available():\n    print(\"Cuda installed! Running on GPU!\")\n    device = 'cuda'\nelse:\n    print(\"No GPU available!\")","execution_count":32,"outputs":[{"output_type":"stream","text":"Cuda installed! Running on GPU!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# means = [0.485, 0.456, 0.406]\n# stds = [0.229, 0.224, 0.225]\n# transform_vgg16_train = Compose([\n#     # ToPILImage(),\n#     Resize(224),\n#     RandomApply([RandomChoice([RandomCrop(size=[224, 224], padding=10), \n#                                RandomAffine(0, translate=(0.01, 0.01))])]), # choose one or 0 transforms that make the image smaller\n#     RandomApply([RandomChoice([RandomHorizontalFlip(), RandomRotation(10)])]), # choose one or zero transforms to rotate or flip the image\n#     ToTensor(),\n#     Normalize(mean=means, std=stds), \n# ]) ##Compose different transforms together. PIL is Python Imaging Library useful for opening, manipulating, and saving many different image file formats.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter\nseed = 42\nlr = 1e-2\nmomentum = 0.5\nbatch_size = 36 # Mini-batch mode\ntest_batch_size = 500\nn_epochs = 30","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the dataset from the default folder\n# Tiny-imagenet-200\n# my_data = training = ImageFolder(\"../input/acse-miniproject/train/\",transform = transform_vgg16_train ) #/content/gdrive/My Drive/ACSE4-ML-miniproject/train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_data = ImageFolder(\"../input/acse-miniproject/test/\", transform_vgg16_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_loader = DataLoader(test_data, batch_size=test_batch_size, shuffle=False, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Split validation and training\n# train_size = int(0.9 * len(my_data))\n# validation_size = len(my_data) - train_size\n# train_dataset, validation_dataset = random_split(my_data, [train_size, validation_size])\n# # DataLoader\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n# validation_loader = DataLoader(validation_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # First import resnet from models, it is pretrained on imagenet and we will keep those weights to detect features\n# # vgg13 = torchmodels.vgg13(pretrained=True)\n# vgg11 = torchmodels.vgg11(pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_size = 299\nfeature_extract = True\n\nmodel_ft = torchmodels.inception_v3(pretrained=True)\nfor param in model_ft.parameters():\n        param.requiresgrad = False\n\nmodel_ft.AuxLogits.fc = nn.Linear(768, 200)\nmodel_ft.fc = nn.Linear(2048,200)\nmodel_ft.aux_logits = False\n\nmodel_ft=model_ft.cuda()\nprint(\"params to update:\")\nparams_to_update = []\nfor name, param in model_ft.named_parameters():\n    if param.requires_grad == True:\n        params_to_update.append(param)\n        print(\"\\t\", name)\noptimizer = torch.optim.Adam(params_to_update, lr=lr)\nloss_func = nn.CrossEntropyLoss().cuda()\n\n\n        \n# set_parameter_requires_grad(model_ft, feature_extract)\n# # Handle the auxilary net\n# num_ftrs = model_ft.AuxLogits.fc.in_features\n# model_ft.AuxLogits.fc = nn.Linear(num_ftrs, 200)\n#         # Handle the primary net\n# num_ftrs = model_ft.fc.in_features\n# model_ft.fc = nn.Linear(num_ftrs,200)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_ft)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_transforms = transforms.Compose([\n        transforms.RandomResizedCrop(input_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \ntest_transforms = transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data = training = ImageFolder(\"../input/acse-miniproject/train/\",transform =data_transforms ) #/content/gdrive/My Drive/ACSE4-ML-miniproject/train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = ImageFolder(\"../input/acse-miniproject/test/\", test_transforms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = DataLoader(test_data, batch_size=test_batch_size, shuffle=False, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split validation and training\ntrain_size = int(0.9 * len(my_data))\nvalidation_size = len(my_data) - train_size\ntrain_dataset, validation_dataset = random_split(my_data, [train_size, validation_size])\n# DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(validation_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Freeze the weights in the earlier layers:\n# for param in vgg16.parameters():\n#     param.requires_grad = False\n\n# print(vgg16)\n\n# vgg16 has 16 layers with the last 4 are fc layers\n# unfreeze the last layer so it learns on our dataset\n# layer_num = 0\n# for child in vgg11.children():\n#   layer_num += 1\n#   if layer_num < 11:\n#       for param in child.parameters():\n#           param.requires_grad = False\n# # vgg16.fc = nn.Linear(num_ftrs, 10) # change number of output classes\n# num_classes = 200\n# num_ftrs = vgg11.classifier[6].in_features\n# vgg11.classifier[6] = nn.Linear(num_ftrs,num_classes)# change number of output classes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, optimizer, criterion, data_loader):\n    model.train()\n    train_loss, train_accuracy = 0, 0\n    for X, y in data_loader:\n        X, y = X.to(device), y.to(device)\n        optimizer.zero_grad()\n        a2 = model(X.view(-1, 3, 299, 299)) # when transferring vgg later on, it takes 224x224 inputs rather than 32x32\n        loss = criterion(a2, y)\n        loss.backward()\n        train_loss += loss*X.size(0)\n        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n        optimizer.step()\n         \n    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)\ndef validate(model, criterion, data_loader):\n  model.eval()\n  validation_loss, validation_accuracy = 0., 0.\n  for X, y in data_loader:\n    with torch.no_grad():\n      X, y = X.to(device), y.to(device)\n      a2 = model(X.view(-1, 3, 299, 299)) # when transferring vgg later on, it takes 224x224 inputs rather than 32x32\n      loss = criterion(a2, y)\n      validation_loss += loss*X.size(0)\n      y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n      validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n      \n  return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)\n\ndef evaluate(model, data_loader):\n  model.eval()\n  ys, y_preds = [], []\n  for X, y in data_loader:\n    with torch.no_grad():\n      X, y = X.to(device), y.to(device)\n      a2 = model(X.view(-1, 3, 299, 299)) # when transferring vgg later on, it takes 224x224 inputs rather than 64x64\n      y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n      ys.append(y.cpu().numpy())\n      y_preds.append(y_pred.cpu().numpy())\n            \n  return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)\n\ndef predict(model, data_loader):\n  model.eval()\n  files, y_preds = [], []\n  for X, y, z in data_loader:\n      with torch.no_grad():\n        X, y = X.to(device), y.to(device)\n        a2 = model(X.view(-1, 3, 299, 299)) # when transferring vgg later on, it takes 224x224 inputs rather than 64x64\n        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n        y_preds.append(y_pred.cpu().numpy())\n        files.append(z)\n            \n  return np.concatenate(y_preds, 0), np.concatenate(files, 0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# model = vgg11.to(device)\nmodel = model_ft.to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\nliveloss = PlotLosses()\nfor epoch in range(6):\n    logs = {}\n    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n    \n    logs['' + 'log loss'] = train_loss.item()\n    logs['' + 'accuracy'] = train_accuracy.item()\n    \n    validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n   \n    logs['val_' + 'log loss'] = validation_loss.item()\n    logs['val_' + 'accuracy'] = validation_accuracy.item()\n    \n    liveloss.update(logs)\n    liveloss.draw()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_accuracy = validate(model, criterion, test_loader)    \nprint(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy.item())\nprint(\"\")\n\ny_pred, y_true = evaluate(model, validation_loader)\n\nf1 = f1_score(y_true, y_pred, average=\"macro\")\nprint(\"f1_score=\",f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred, y_true = evaluate(model, validation_loader)\n\nf1 = f1_score(y_true, y_pred, average=\"macro\")\nprint(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_predictions, my_files = predict(resnet2, test_loader)\n\nmy_files_clean = [my_files[i][38:] for i in range(len(my_files))]\n\nwith open('submission.csv', 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n    wr = csv.writer(myfile)\n    wr.writerow((\"Filename\", \"Label\"))\n    wr.writerows(zip(my_files_clean,my_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the augmented model:\nmodel_save_name = 'GoogleNet_inception_v3.pth'\npath = F\"./{model_save_name}\" \ntorch.save(model.state_dict(), path)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}